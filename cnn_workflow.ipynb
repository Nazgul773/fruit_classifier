{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-01T17:30:44.274252Z",
     "start_time": "2024-06-01T17:30:44.260228Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from preprocessors import preprocess\n",
    "\n",
    "from models import FruitVeggieClassifier0Acc, FruitVeggieClassifier55Acc, PretrainedFruitVeggieClassifier"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:30:44.306394Z",
     "start_time": "2024-06-01T17:30:44.290855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameter\n",
    "num_classes = 36\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "id": "1d376e7da9c6dde8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Augmentation and Normalization",
   "id": "29040a8f67a28fee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:30:44.322429Z",
     "start_time": "2024-06-01T17:30:44.306394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Define the image transformation with data augmentation\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.RandomResizedCrop(128),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#     transforms.RandomRotation(degrees=15),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406, 0.5], std=[0.229, 0.224, 0.225, 0.5])\n",
    "# ])\n",
    "# \n",
    "# val_test_transform = transforms.Compose([\n",
    "#     transforms.Resize(144),\n",
    "#     transforms.CenterCrop(128),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406, 0.5], std=[0.229, 0.224, 0.225, 0.5])\n",
    "# ])"
   ],
   "id": "c79a65078181d1b7",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:30:44.338195Z",
     "start_time": "2024-06-01T17:30:44.322429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Define the data loaders\n",
    "# def pil_loader(path):\n",
    "#     with open(path, 'rb') as f:\n",
    "#         img = Image.open(f)\n",
    "#         return img.convert('RGBA')"
   ],
   "id": "68fcfbe4d395b9df",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:30:44.354232Z",
     "start_time": "2024-06-01T17:30:44.338195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class ImageFolderWithPaths(ImageFolder):\n",
    "#     \"\"\"Custom dataset that includes image file paths. Extends torchvision.datasets.ImageFolder\"\"\"\n",
    "# \n",
    "#     def __getitem__(self, index):\n",
    "#         # This is what ImageFolder normally returns \n",
    "#         original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "#         # The image file path\n",
    "#         path = self.imgs[index][0]\n",
    "#         # Make a new tuple that includes original and the path\n",
    "#         tuple_with_path = (original_tuple + (path,))\n",
    "#         return tuple_with_path"
   ],
   "id": "21f702e5398cce50",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:30:44.370264Z",
     "start_time": "2024-06-01T17:30:44.354232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data_dir = 'data'\n",
    "# train_data = ImageFolder(root=os.path.join(data_dir, 'train'), transform=train_transform, loader=pil_loader)\n",
    "# val_data = ImageFolder(root=os.path.join(data_dir, 'val'), transform=val_test_transform, loader=pil_loader)\n",
    "# benchmark_data = ImageFolderWithPaths(root=os.path.join(data_dir, 'test'), transform=val_test_transform,\n",
    "#                                       loader=pil_loader)\n",
    "# \n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "# benchmark_loader = DataLoader(benchmark_data, batch_size=16, shuffle=False)  #, collate_fn=my_collate)"
   ],
   "id": "c58b8af09d1a4f14",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Architecture",
   "id": "8fd11be9aba65ea3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:30:44.386731Z",
     "start_time": "2024-06-01T17:30:44.370264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Option to build the model from scratch or load a saved model\n",
    "def build_or_load_model(load_model, model):\n",
    "    path = model.model_path\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    start_epoch = 0\n",
    "\n",
    "    if load_model and os.path.exists(path):\n",
    "        try:\n",
    "            checkpoint = torch.load(path)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            print(\"Model loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "    else:\n",
    "        print(\"Model built from scratch.\")\n",
    "\n",
    "    return model, optimizer, scheduler, start_epoch"
   ],
   "id": "303587d5357d7b93",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:30:44.400304Z",
     "start_time": "2024-06-01T17:30:44.386731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_validate(train_loader, val_loader, model, optimizer, scheduler, start_epoch, num_epochs=num_epochs, patience=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    epochs_without_improvement = 0\n",
    "    save_path = model.model_path\n",
    "    \n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels, _ in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # zero the parameter gradients\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(images) \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch + 1}/{start_epoch + num_epochs}], Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels, _ in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "        print(f'Current learning rate: {current_lr:.6f}')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch = epoch\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'loss': running_loss,\n",
    "            }, save_path)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\n",
    "                    f'Early stopping at epoch {epoch + 1}. Best validation loss: {best_val_loss:.4f} at epoch {best_epoch + 1}.')\n",
    "                break"
   ],
   "id": "725be25848e20a49",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:30:44.414400Z",
     "start_time": "2024-06-01T17:30:44.400304Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b4f4398611f6623",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:30:44.460572Z",
     "start_time": "2024-06-01T17:30:44.414400Z"
    }
   },
   "cell_type": "code",
   "source": "init_model = PretrainedFruitVeggieClassifier(num_classes=num_classes).to(device)",
   "id": "968a6b82138cdc81",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:30:44.474366Z",
     "start_time": "2024-06-01T17:30:44.460572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_type = init_model.model_type\n",
    "load_model = True  # Change this to True if you want to load a pre-trained model\n",
    "train_loader, val_loader, benchmark_loader = preprocess(model_type=model_type)"
   ],
   "id": "10aa5ded12d1e791",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:30:44.544363Z",
     "start_time": "2024-06-01T17:30:44.474366Z"
    }
   },
   "cell_type": "code",
   "source": "net, optimizer, scheduler, start_epoch = build_or_load_model(load_model=load_model, model=init_model)  ",
   "id": "a06b3a3cff534185",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:30:45.189960Z",
     "start_time": "2024-06-01T17:30:44.544363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_and_validate(train_loader, \n",
    "                   val_loader, \n",
    "                   net, \n",
    "                   optimizer, \n",
    "                   scheduler, \n",
    "                   start_epoch, \n",
    "                   start_epoch + num_epochs)"
   ],
   "id": "2f00055a2c58a833",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain_and_validate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m                   \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mstart_epoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mstart_epoch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[20], line 11\u001B[0m, in \u001B[0;36mtrain_and_validate\u001B[1;34m(train_loader, val_loader, model, optimizer, scheduler, start_epoch, num_epochs, patience)\u001B[0m\n\u001B[0;32m      9\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     10\u001B[0m running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels, _ \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m     12\u001B[0m     images, labels \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     14\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# zero the parameter gradients\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\fruit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\PycharmProjects\\fruit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\PycharmProjects\\fruit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\fruit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    256\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    315\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\fruit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    170\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\fruit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    170\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\fruit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:191\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    186\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    187\u001B[0m             \u001B[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001B[39;00m\n\u001B[0;32m    188\u001B[0m             \u001B[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001B[39;00m\n\u001B[0;32m    189\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]\n\u001B[1;32m--> 191\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(default_collate_err_msg_format\u001B[38;5;241m.\u001B[39mformat(elem_type))\n",
      "\u001B[1;31mTypeError\u001B[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing the Model",
   "id": "2f743f516c8bfe99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def evaluate(model, test_loader):\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     data = []\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels, paths in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "# \n",
    "#             # Save the image paths and class labels\n",
    "#             for path, label in zip(paths, predicted):\n",
    "#                 class_name = benchmark_data.classes[label]  # Get the class name\n",
    "#                 data.append([Path(path).stem, class_name])\n",
    "# \n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f'Accuracy: {accuracy:.2f}%')\n",
    "# \n",
    "#     df = pd.DataFrame(data, columns=['id', 'ClassLabel'])\n",
    "#     model_path = model.model_path\n",
    "#     model_name = os.path.splitext(os.path.basename(model_path))[0]\n",
    "#     output_dir = os.path.join(\"results\", os.path.dirname(model_path), model_name)\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "#     output_file = os.path.join(output_dir, f\"result_{current_time}.csv\")\n",
    "#     df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "#     print(f\"CSV file has been created: {output_file}\")"
   ],
   "id": "26ffc2ae0cf83e41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:30:45.189960Z",
     "start_time": "2024-06-01T17:30:45.189960Z"
    }
   },
   "cell_type": "code",
   "source": "# evaluate(net, benchmark_loader)",
   "id": "e5eec472ad8807ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "- https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "- https://pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/"
   ],
   "id": "869bef9d4fcec19f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "59d9e180c6aafa96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cc73be4496066063",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
